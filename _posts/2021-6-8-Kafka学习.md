---
layout: post
author: LIU,HONGYANG
tags: [Kafka]
---



##### 解决问题？什么是kafka?



1.Scala语言开发的多分区，多副本基于Zookeeper协调的**分布式消息系统** 

2.目前，Kafka被定位为一个分布式流式处理平台





##### 为什么可以做到？

kafka的特性



##### 在什么地方用到？



（1）**日志收集**（重点），用Kafka收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如Hadoop、Hbase、Solr等

（2）消息系统

（3）用户活动跟踪

（4）运营指标

（5）流式处理：Spark Streaming



##### kafka基本概念

Topics:类似数据库中的表，kafka是表名

Kafka Brokers: broker和partition相关联的概念，其中broker数量应该多余partition数量，尽量避免broker的数量小于partition数量的情况，否则会导致数据不均衡情况



![image-20210607105128482](https://tva1.sinaimg.cn/large/008i3skNly1gr9ihtiriwj30s10ddgsq.jpg)



replica: 一个partition有多个副本

leader: 一个leader，多个follower，数据先写入leader中，同步到follower，当leader挂掉时，从follower选举出leader

consumer:消费从leader读取



![image-20210607110936162](https://tva1.sinaimg.cn/large/008i3skNly1gr9j0krjpwj30j00dk433.jpg)



##### 如何在机器上单机配置？



基本要求：java环境



参考`/root/hadoop/apache-flume-1.9.0-bin/conf`中apache-flume.sh得到jre路径

```
export JAVA_HOME=/etc/alternatives/jre_1.8.0
vim /etc/profile #vim profile文件
source /etc/profile #生效
```



```
export JAVA_HOME=/etc/alternatives/java_sdk_1.8.0
export PATH=$JAVA_HOME/bin:$PATH
```



下载`zookeeper` 



```
#找到zoo.cfg文件
/root/hadoop/apache-zookeeper-3.7.0-bin/conf
```



```
# The number of milliseconds of each tick
#心跳时间12
tickTime=2000
# The number of ticks that the initial
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just
# example sakes.
#数据目录
dataDir=~/hadoop/apache-zookeeper-3.7.0-bin/data
# the port at which the clients will connect
#服务对外端口
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1

## Metrics Providers
#
# https://prometheus.io Metrics Exporter
#metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider
#metricsProvider.httpPort=7000
#metricsProvider.exportJvmInfo=true
```



修改完成后，运行`zookeeper` 

```iJAVA
#启动
/bin/zkServer start
#查看
jps -l
```



##### **遇到bug**



```
[root@s2-vm-02-39 bin]# jps
bash: jps: command not found..
```



`问题原因：`

环境变量配置有误，发现未成功(错误配置如下)

```
export JAVA_HOME=/etc/alternatives/jre_1.8.0
export PATH=$JAVA_HOME/bin:$PATH
```

根本原因在于：centos自带open-jdk无该命令，需要重新下载



解决方式二：



[下载open-jdk](https://www.digitalocean.com/community/tutorials/how-to-install-java-on-centos-and-fedora) (ps: ctrl+command+space)可以打开特殊字符



```
export JAVA_HOME=/etc/alternatives/java_sdk
export PATH=$JAVA_HOME/bin:$PATH
```

  

默认server.propoties

```
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# see kafka.server.KafkaConfig for additional details and defaults

############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=0

############################# Socket Server Settings #############################

# The address the socket server listens on. It will get the value returned from
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
listeners=PLAINTEXT://:9092

# Hostname and port the broker will advertise to producers and consumers. If not set,
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
#advertised.listeners=PLAINTEXT://your.host.name:9092

# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3

# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600


############################# Log Basics #############################

# A comma separated list of directories under which to store log files
log.dirs=/opt/kafka_2.13-2.8.0/logs

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=1

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

############################# Log Flush Policy #############################

# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168

# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
zookeeper.connect=localhost:2181

# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=18000


############################# Group Coordinator Settings #############################

# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
group.initial.rebalance.delay.ms=0
```



启动kafka

```
 bin/kafka-server-start.sh config/server.properties
 #后台启动
 bin/kafka-server-start.sh -daemon  config/server.properties
```



检查启动是否成功

```
[root@s2-vm-02-39 apache-zookeeper-3.7.0-bin]# jps -l
26856 org.apache.zookeeper.server.quorum.QuorumPeerMain
4091 sun.tools.jps.Jps
27885 /opt/lina/Lina.jar
3773 kafka.Kafka
```



##### 如何创建消息 的生产与消费?



- create:

```
[root@s2-vm-02-39 kafka_2.13-2.8.0]# bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic hongyang  --partitions 2 --replication-factor 1
Created topic hongyang.
```



- list:

```
[root@s2-vm-02-39 kafka_2.13-2.8.0]# bin/kafka-topics.sh --zookeeper localhost:2181 --list  
hongyang
```



- describe:

```
[root@s2-vm-02-39 kafka_2.13-2.8.0]# bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic hongyang
Topic: hongyang	TopicId: dkr-1LyaTq--tuA3B6-2eg	PartitionCount: 2	ReplicationFactor: 1	Configs:
	Topic: hongyang	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
	Topic: hongyang	Partition: 1	Leader: 0	Replicas: 0	Isr: 0
```



- delete

```
bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic hongyang 
```



- 消费端接收消息



```
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic hongyang
```



```
[root@s2-vm-02-39 kafka_2.13-2.8.0]# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic hongyang
hello
world
```





- 生产端发送消息



```
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic hongyang
```



```
[root@s2-vm-02-39 kafka_2.13-2.8.0]# bin/kafka-console-producer.sh --broker-list localhost:9092 --topic hongyang
>hello
>world
>
```


##### Kafka程序构建

kafka编程-使用idea创建项目

略。。。



1.kafka消息发送流程

序列化器

分区器

主题A 主题B
分区0 分区1



发送原理剖析



消息接收参数设置

订阅主题和分区

消费消息

消息丢失

自动提交





##### 待定学习。。。

b站视频


同步、异步提交

Kafka有四个核心API,它们分别是：

Producer API, 它允许应用程序向一个或多个topics上发送消息记录

Consumer API,允许应用程序订阅一个或多个topics并处理为其生成的记录流

Streams API,它允许应用程序作为流处理器，从一个或多个主题中消费输入流并为其生成输出流，有效的将输入流转换为输出流

Connector API,它允许构建和运行将Kafka主题连接到现有应用程序或数据系统的可用生产者和消费则。例如，关系数据库的连接器可能会捕获对表的所有更改。

![](https://tva1.sinaimg.cn/large/008i3skNly1grd71iz0nsj30yw0scdp1.jpg)


broker端配置

- broker.id 

每个kafka broker都有一个唯一的标识来表示，这个唯一的标识符即是broker.id，它的默认值是0.这个值在kafka集群中必须是唯一的，这个值可以任意设定

- port

如果使用配置样本来启动kafka，它会监听9092端口。修改port配置参数可以把它设置成任意的端口。要注意，如果使用1024以下的端口，需要使用root权限启动kafka



- zookeeper.connect

- log.dirs
日志数据存放的目录，如果没有配置则使用log.dir，建议此项配置


- listeners

监听列表，broker对外提供服务时绑定的IP和端口。多个以逗号隔开，如果监听器不是一个安全的协议，
`listener.security.protocol.map`也必须配置。主机名称设置0.0.0.0绑定所有的接口，主机名称为空则
绑定默认的接口。

如：PLAINTEXT://myhost:9092, SSL://:9091 CLIENT://0.0.0.0:9092, REPLICATION://loalhost:9093 


- num.recovery.threads.per.data.dir

服务器正常启动，用于打开每个分区的日志片段

服务器崩溃后重启，用于检查和截断每个分区的日志片段

服务器正常关闭，用于关闭日志片段


- auto.create.topics.enable

默认情况下，kafka会使用三种方式来自动创建主题

当一个生产者开始往主题写入消息时，


当一个消费者开始从主题读取消息时，


当任意一个客户端向主题发送元数据时

auto.create.topics.enable参数一般设置为false.

##### 主题默认配置

kafka为新创建的主题提供了很多默认配置参数，

- num.partitions

num.partitons参数指定了新创建的主题需要包含多少个分区。如果启用了主题自动创建功能（该功能是默认启用的），主题分区的个数就是该参数指定的值。该参数的默认值是1。注意：我们可以增加主题分区的个数，但不能减少分区的个数。


- default.replication.factor

表示kafka保存消息的副本，如果一个副本失效了，另一个还可以继续提供服务. default.replication.factor的默认值为1，这个参数在你启用了主题自动创建功能后有效。

- log.retention.ms

kafka通常根据时间来决定数据可以保留多久。默认使用log.retention.hours参数来配置时间，默认是168个小时，也就是一周。除此之外，还有两个参数log.retention.minutes和log.retention.ms。这三个参数作用是一样的，都是决定消息多久以后被删除，推荐使用log.retention.ms


- log.retention.bytes

和logretention.ms类似，表示该主题最多保留8GB数据.

- log.segment.bytes

上述的日志都是作用在日志片段上，而不是作用在单个消息上。当消息到达broker时，它们被追加到分区的当前日志片段上，当日志片段大小到达log.segment.bytes指定上限（1GB）时，当前日志片段就会被关闭，一个新的日志片段被打开。如果一个日志片段被关闭，就开始等待过期。这个参数的值越小，就越会频繁的关闭和分配新文件，从而降低磁盘写入的整体效率。

- log.segment.ms

log.segment.ms这个参数指定日志多长时间被关闭的参数和，log.segment.ms和log.retention.bytes也不存在互斥问题。日志片段会在大小或时间到达上线时被关闭，就看哪个条件先得到满足。


- message.max.bytes

broker通过设置`message.max.bytes`参数来限制单个消息的大小，默认是1000000,也就是1MB，如果生产者尝试发送的消息超过这个大小，不仅消息不会被接收，还会收到broker返回的错误消息。

跟其它于字节相关的配置参数一样，该参数值的是压缩后的消息大小，也就是说，只要压缩后的消息小于`message.max.bytes`，那么消息的实际大小可以大于这个值

- retention.ms

规定了该主题消息被保存的时常，默认是7天，即该主题只能保存7天的消息，一旦设置了这个值，它会覆盖掉Broker端的全局参数值。


- retention.bytes

`retebtuib.bytes`:规定了要为该Topic预留多大的磁盘空间。默认为-1，表示无限使用。

##### JVM参数配置

- Java 7

如果Broker所在的CPU资源非常充足，建议使用CMS收集器。启用方法是指定`-XX: +UseCurrentMarkSleepGC`

否则，使用吞吐量收集器，开启方法是`-XX: +UseParallelGC`


- Java 8

G8的主要特性表现在更少的Full GC上面，需要调整的参数更少。

一般G1的调整只需要调整两个参数：

- MaxGCPauseMillis

该参数指定每次垃圾回收默认的停顿时间。该值不是固定的，默认是200ms，每一轮垃圾回收大约200ms


- InitiatingHeapOccupancyPercent

该参数指定了G1启动新一轮垃圾回收之前可以使用的堆内存百分比，默认值是45，这就表明G1在堆使用率到达45之前不会启用垃圾回收。这个百分比包含新生代和老生代




##### Kafka Producer


![](https://tva1.sinaimg.cn/large/008i3skNly1grdci87vylj30u00x8ain.jpg)

在发送ProducerRecord时，我们需要将键值对对象由序列化器转化为字节数组，这样它们才能够在网络上传输。然后消息到达了分区器。

如果发送过程中指定了有效的分区号，那么在发送记录时将使用该分区。如果发送过程中未指定分区，则将使用key的hash函数映射指定
一个分区。如果发送的过程中既没有分区号，则将以循环的方式分配一个分区。选好分区后，生产者就知道向哪个主题和分区发送数据了。


ProducerRecord还有关联时间戳，如果用户没有提供时间戳，那么生产者将会在记录中使用当前的时间作为时间戳。Kafka最终使用
的时间戳取决于topic主题配置的时间戳


- 创建Kafka生产者


要向Kafka写入消息，首先需要创建一个生产者对象，并设置一些属性。Kafka生产者有3个必须的属性

- bootstrap.servers

该属性指定broker的地址清单，地址的格式为host:port。清单里不需要包含所有的broker地址，生产者会从给定的broker里查找到
其它的broker信息。

- key.serializer

broker需要接收到序列化之后的`key/value`值，所以生产者发送的消息需要经过序列化之后才传递给Kafka Broker。生产者需要知道采用
何种方式把Java对象转换为字节数组。key.serializer必须被设置为一个实现了`org.apache.kafka.common.serialization.Serializer`
接口的类，生产者会使用这个类把键对象序列化为字节数组。



- value.serializer

与key.serializer一样，value.serializer指定的类会将值序列化


```
private Properties properties = new Properties();
properties.put("bootstrap.servers","broker1:9092,broker2:9092")
properties.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer")
properties.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer")
properties = new KafkaProducer<String,String>(properties)

```

说明：
- 首先创建了一个Properties对象
- 使用StringSerializer序列化器序列化key/value键值对
- 创建一个新的生产者对象，并为键值设置了恰当的类型，然后把Properties对象传递给他


##### Kafka消息发送

参考：https://www.javatpoint.com/creating-kafka-producer-in-java

1.Kafka Dependencies
2.Logging Dependencies, i.e., SLF4J Logger.


 ```
package chapter1;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;

import java.util.Properties;

public class ProducerFastStart {

    public static void main(String[] args) {
        Properties properties = new Properties();
        //设置集群地址
        properties.put("bootstrap.servers", "broker1:9092,broker2:9092");
        //设置key序列化器
        properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        //设置value序列化器
        properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")

        //设置重试次数
        properties.put(ProducerConfig.RETRIES_CONFIG, 10);

        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);
        ProducerRecord<String, String> record = new ProducerRecord<String, String>("CustomerCountry", "West", "France");


        try {
            producer.send(record);
        } catch (Exception e) {
            e.printStackTrace();
        }

        producer.close();
    }
}

 ```




- 简单消息发送


Kafka最简单的消息发送如下：
```
ProducerRecord<String,String> record = new ProducerRecord<String,String>("CustomerCountry","West","France")
producer.send(record)

```

代码中生产者(producer)的send()方法需要把`ProducerRecord`的对象作为参数进行发送。

![](https://tva1.sinaimg.cn/large/008i3skNly1gre4xohwzlj30pq0hin0q.jpg)


发送成功后，send()方法会返回一个`Future`对象，Future对象的类型是RecordMetadata类型，我们上面这段代码没有考虑返回值，所以没有
生成对应的Future对象，所以没有办法知道消息是否发送成功。如果不是很重要的信息或者对结果不会产生影响的信息，可以使用这种方式
进行发送。


 - 同步发送消息

同步发送消息是对简单发送消息的改进


```
ProducerRecord<String, String> record = new ProducerRecord<String, String>("CustomerCountry", "West", "France");
    try {
        RecordMetadata recordMetadata = producer.send(record).get();
    } catch (Exception e) {
        e.printStackTrace();
    }
```

首先调用sned()方法，然后再调用get()方法等待Kafka响应。如果服务器返回错误，get()方法会抛出异常，如果没有发送错误，我们会得到RecordMetadata对象，可以用它来查看消息记录。

生产者（KafkaProducer）在发送的过程中会出现两类错误：其中一类是重试错误，这类错误可以通过重发消息来解决。比如连接的错误，可以通过再次建立连接来解决；

无`主`错误则可以通过重新为分区选举首领来解决。KafkaProducer被配置为自动重试，如果多次重试后仍无法解决问题，则会抛出重试异常。

另一类错误是无法通过重试来解决，比如`消息过大`对于这类错误，KafkaProducer不会进行重试，直接抛出异常。

- 异步发送消息 


```
package chapter1;

import org.apache.kafka.clients.producer.*;

import java.util.Properties;

public class ProducerFastStart {

    public static void main(String[] args) {
        Properties properties = new Properties();
        //设置集群地址
        properties.put("bootstrap.servers", "broker1:9092,broker2:9092");
        //设置key序列化器
        properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        //设置value序列化器
        properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")

        //设置重试次数
        properties.put(ProducerConfig.RETRIES_CONFIG, 10);

        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);
        ProducerRecord<String, String> preducerRecord = new ProducerRecord<String, String>("CustomerCountry", "West", "France");

        producer.send(preducerRecord,new DemoProducerCallBack());
    }
}

class DemoProducerCallBack implements Callback {
    public void onCompletion(RecordMetadata metadata, Exception exception) {
        if (exception != null) {
            exception.printStackTrace();
        }
    }
}
```

首先实现回调需要定义一个实现`org.apache.kafka.clents.producer.Callback` 的类，这个接口只有一个
`onCompletion`方法。如果Kafka返回一个错误onCompletion方法会抛出一个非空(non null)异常。






```

package chapter1;

import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class ConsumerFastStart {
    private static final String  brokerList = "192.168.19.128:9092";
    private static final String topic = "heima";
    private static final String groupId = "group.demo";

    public static void main(String[] args) {
        Properties properties = new Properties();

        //设置集群地址
        properties.put("bootstrap.servers", "broker1:9092,broker2:9092");
        //设置key序列化器
        properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        //设置value序列化器
        properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")

        properties.put("group.id",groupId);

        KafkaConsumer<String,String> consumer = new KafkaConsumer<String,String>(properties);

        consumer.subscribe(Collections.singletonList(topic));

        while (true){
            ConsumerRecords<String,String> consumerRecords = consumer.poll(Duration.ofMillis(1000));
            for(ConsumerRecords<String,String> record:consumerRecords){
                System.out.println(consumerRecords.value());
            }
        }

    }
}
```


优化与总结

```
package chapter1;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringSerializer;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class ConsumerFastStart {
    private static final String  brokerList = "192.168.19.128:9092";
    private static final String topic = "heima";
    private static final String groupId = "group.demo";

    public static void main(String[] args) {
        Properties properties = new Properties();

        //设置集群地址
        //properties.put("bootstrap.servers", "broker1:9092,broker2:9092");
        //设置key序列化器
        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "broker1:9092,broker2:9092");
        //properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        //设置value序列化器
        //properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        //properties.put("group.id",groupId);
        properties.put(ConsumerConfig.GROUP_ID_CONFIG,groupId);

        KafkaConsumer<String,String> consumer = new KafkaConsumer<String,String>(properties);

        consumer.subscribe(Collections.singletonList(topic));

        while (true){
            ConsumerRecords<String,String> consumerRecords = consumer.poll(Duration.ofMillis(1000));
            for(ConsumerRecords<String,String> record:consumerRecords){
                System.out.println(consumerRecords.value());
            }
        }

    }
}

```

##### 第2章 生产者详解


![](https://tva1.sinaimg.cn/large/008i3skNly1greb5sfddej30qe0nw0wy.jpg)

序列化的作用：把对象序列化成字节数组

发送类型：

- 同步发送

```
Future<RecordMetadata> send = producer.send(record);
RecordMetadata recordMetadata = send.get();
System.out.println(recordMetadata.topic());
System.out.println(recordMetadata.partition());
System.out.println(recordMetadata.offset());

```



- 异步发送


异步发送相当于重启了一个线程，不阻塞当前线程的处理

```
producer.send(record,new Callback(){
        public void onCompletion(RecordMetadata metadata,Exception exception){
            if(exception==null){
                    System.out.println(metadata.partition()+":"+metadata.offset());
            }
        }
 }
);
```



- 序列化器

可以通过对对象的改写，传送自定义的序列化器

- 分区器


- 拦截器

生产者拦截器可以用在消息发送前左一些准备工作。

使用场景：

1.按照某个规则过滤掉不符合要求的消息

2.修改消息的内容

3.统计类需求

![](https://tva1.sinaimg.cn/large/008i3skNly1grebv12ti2j31h80bcgwq.jpg)

KafkaProducer->producerInterceptors->Serializer->Partitioner


- 其他生产者参数

ack=0,生产者在成功写入消息之前不会等待任何来自服务器的响应。如果出现问题生产者是感知不到的，
消息就丢失了。不过因为生产者不需要等待服务器响应，所以他可以以网络能过支持的最大速度发送消息，从而达到很高的吞吐量。

ack=1，默认值为1，只要集群的首领节点收到消息，生产者就会收到一个来自服务器的成功响应。

ack=-1,只有当所有参与复制的节点都收到消息时，生产者会收到一个来自服务器的成功响应，这种模式是最安全的，他可以保证不止一个服务器收到消息。

```
props.put(ProducerConfig.ACKS_CONFIG,"0")
```


retires:

生产者从服务器收到的错误可能是临时性的错误。如果达到了retries设置的次数，生产者会放弃重试并返回错误。


batch.size:

当有多个消息要被发送到同一分区时，生产者会把它们放在同一批次里。该参数制定了一个批次可以使用的内存
大小，按照字节数计算，而不是消息个数。


max.request.size:

该参数用于控制生产者发送的请求大小，它可以指定能发送的单个消息的最大值，也可以指单个请求里所有消息的总大小。


第3章 消费者详解

- 消费者、消费组

![](https://tva1.sinaimg.cn/large/008i3skNly1gred6gwwtgj30uy0c8wfw.jpg)


需要指定消费组，
```
properties.put(ConsumerConfig.GROUP_ID_CONFIG,groupId);
```


 - 消息接收参数设置

 ```
 public static Properties initConfig(){
    Properties props = new Properties();
    //与KafkaProducer总设置保持一致
    props.put("key.deserializer","org.apache.kafka.common.serialization.StringDeserializer");
    props.put("value.deserializer","org.apache.kafka.common.serialization.StringDeserializer");

    //必填参数，该参数和KafkaProducer中的相同，制定连接Kafka集群所需的broker地址清单，可以设置一个或多个

    props.put("bootstrap.servers",brokerList);
    //消费者隶属于的消费组，默认为空，如果设为空，则会抛出异常，这个参数要设置成具有一定业务含义的名称
    props.put("group.id",groupId);

    //指定kafkaConsumer对应的客户端ID,默认为空，如果不设置KafkaConsumer会自动生成一个非空字符串
    props.put("clinet.id","consumer.client.id.demo");

    return props;
}
 ```


待看：
订阅主题和分区









