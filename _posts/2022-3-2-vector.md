---
layout: post
author: LIU,HONGYANG
tags: [大数据组件]
---
# Vector介绍

Vector 是一种高性能的可观察性数据管道，可让组织控制其可观察性数据。收集、转换所有日志、指标和跟踪信息，并将其路由到您今天想要的任何供应商以及您明天可能想要的任何其他供应商。 Vector 可以在您需要的地方实现显着的成本降低、新颖的数据丰富和数据安全，而不是在您的供应商最方便的地方。开源，比所有替代方案快 10 倍。

- Events:
代表Vector中数据单元

- Components:

  组件是源、转换和接收器的通用术语。组件摄取、转换和路由事件。您可以组合组件来创建拓扑。
  - *Sources*
  如果不能摄取数据，Vector 就不会很有用。源定义了 Vector 应该从哪里提取数据，或者它应该如何接收推送给它的数据。拓扑可以有任意数量的源，并且当它们摄取数据时，它们会继续将其规范化为事件（请参阅下一节）。这为轻松一致地处理数据奠定了基础。源示例包括文件、系统日志、statsd 和标准输入。
  - *Transforms* 转换负责在 Vector 传输事件时对事件进行变异。这可能涉及解析、过滤、采样或聚合。您可以在管道中进行任意数量的转换，它们的组合方式取决于您。

  - *Sinks* 接收器是事件的目的地。每个接收器的设计和传输方法由与之交互的下游服务决定。例如，套接字接收器流式传输单个事件，而 aws_s3 接收器缓冲和刷新数据。

- Pipeline:

  管道是组件的有向无环图。每个组件都是图中的一个节点，具有有向边。数据必须沿一个方向流动，从源到汇。组件可以产生零个或多个事件。

- Buffers:

  接收器尝试尽可能快地发送事件。如果他们无法跟上，他们有一个可配置的缓冲区来保存事件，直到它们可以被发送。默认情况下，Vector 使用内存缓冲区，但也可以使用磁盘缓冲区。一旦缓冲区填满，行为是可配置的。

- Roles: 角色是 Vector 填充以创建端到端管道的部署角色。
  - *Agent* 代理角色设计用于将 Vector 部署到边缘，通常用于数据收集。
  - *Aggregator* 聚合器角色旨在收集和处理来自多个上游来源的数据。这些上游源可以是其他 Vector 代理或非 Vector 代理，例如 Syslog-ng。


- Topology

  拓扑是将 Vector 部署到您的基础架构中的最终结果。拓扑可能像将 Vector 部署为代理一样简单，也可能像将 Vector 部署为代理并通过多个 Vector 聚合器路由数据一样复杂。

# Vector结构

vector主要有三种结构，分布式，集中式和流式，主要介绍两种，

- 集中式：
![](https://tva1.sinaimg.cn/large/e6c9d24ely1gzvk7azj62j214y0e6myh.jpg)

集中式平衡了简单，稳定，以及可控。对于许多用例，一个集中式配置拓扑图式分布式和流式的妥协，它提供
了许多流式拓扑的优点，例如明确的职责分离，不需要流式的过多开销。 通常结合vector和Apache kafka
或Apache Pulsar使用。


- 流式：
![](https://tva1.sinaimg.cn/large/e6c9d24ely1gzvjzvp2pej215m0ewq4l.jpg)

流式主要用于kafka，作为流式的消费者。既可以向kafka发送数据，也可以从kafka拉数据，放在s3云存储
上。主要优点有：
保持持久性和依赖性，高效性，能够重新流式传输，职责分离。


# Vector安装

以centos系统为例，配置参数如下：

```shell
Static hostname: s2-vm-02-39
      Icon name: computer-vm
        Chassis: vm
     Machine ID: 00f00640ab8a489aa772a5c90058bb07
        Boot ID: f267bf7f8bd648ed90bdc07d3ad8594b
 Virtualization: kvm
Operating System: CentOS Linux 7 (Core)
    CPE OS Name: cpe:/o:centos:centos:7
         Kernel: Linux 3.10.0-1160.31.1.el7.x86_64
   Architecture: x86-64
```

- 安装：

```shell
curl -1sLf 'https://repositories.timber.io/public/vector/cfg/setup/bash.rpm.sh' \
  | sudo -E bash
```

```shell
sudo yum install vector
```

- 升级:

```shell
sudo yum upgrade vector
```


- 卸载:

```shell
sudo yum remove vector
```

# 数据模型

![](https://tva1.sinaimg.cn/large/e6c9d24ely1gzvon3uifoj217y0oyjut.jpg)

一个 `log event`在vector中是一个结构化表示，它包含任意描述这个event的字段。


Vector 的一个关键原则是模式中立性。这确保 Vector 可以与任何模式一起使用，随着您的需求的发展支持旧模式和未来模式。 Vector 不需要任何特定的字段，每个组件都记录了它提供的字段。

```json
{
  "custom": "field",
  "host": "my.host.com",
  "message": "Hello world",
  "timestamp": "2020-11-01T21:15:47+00:00"
}
```

### Vector Remap Language

VRL是一个面向表达式的语言，

### Configuration

#### Sources

vector支持多种source数据源，支持从中采集日志。
![](https://tva1.sinaimg.cn/large/e6c9d24ely1h015jpr9l0j215q0u0q6y.jpg)

这里举比较常见的的source来源，比如File, Stdin, 以及Kafka

- **File**

首先是File, File最常见的作用是监控日志的更新。具体设置方式见官方文档[File](https://vector.dev/docs/reference/configuration/sources/file/)

```shell
[sources.my_source_id]
type = "file"
ignore_older_secs = 600
include = [ "/var/log/**/*.log" ]
read_from = "beginning"
```

1.sources名称为`my_source_id`
2.source类型为file
3.日志需要忽略的时间，设置为600秒
4.read_from设置为`beginning`，从头开始读取日志数据。

`vector.toml`配置文件如下：

```shell
[sources.syslog]
type = "file"
ignore_older_secs= 600
include = ["./test.log"]
read_from = "end"

[transforms.remap]
inputs = ["syslog"]
type = "remap"

source = '''
   if contains!(.message, "test")!= true {
      del(.message)
   }
'''

[sinks.emit_syslog]
inputs = ["remap"]
type = "console"
target = "stdout"
encoding.codec = "json"
```

从`test.log`中读取数据， 在transform中判断日志是否包含test，如果不包含，就过滤日志，不打印。只输出包含test的日志。

![](https://tva1.sinaimg.cn/large/e6c9d24ely1h017dp2wtlj216405u0um.jpg)

可以看到，`.message`中都包含test


- **Stdin**

```shell
[sources.my_source_id]
type = "stdin"
```

`stdin`是从客户端输入，所以在执行过程中，可以使用
```shell
echo 'Hello world!' | vector --config ./vector.toml
```
`echo`运行命令进行输出

```shell
[sources.generate_syslog]
type = "stdin"

[transforms.remap_syslog]
inputs = ["generate_syslog"]
type = "remap"
source = '''
	. = parse_key_value!(.message)
'''

[sinks.emit_syslog]
inputs = ["remap_syslog"]
type = "console"
encoding.codec = "json"
```
- **Kafka**

第三个需要理解的是kafka, Kafka作为消费，连接kafka集群，消费的topic是`test2`

kafka需要配置的参数有`bootstrap_servers`, `group_id`和`topics`
`bootstrap_servers`用于填写kafka的host和port地址，需要使用逗号分开。

`group_id`是必须的参数，需要唯一

`topics`是消费的kafka topic，可以消费多个topic，并用逗号分隔开。


```shell
[sources.kafka_log]
type = "kafka"
bootstrap_servers = "172.18.2.39:9092"
group_id = "consumer-group-name"
topics = ["test2"]
auto_offset_reset= "latest"
```

#### Transforms



#### Sinks

- **Kafka**

```shell
[sinks.my_sink_id]
type = "kafka"
inputs = [ "my-source-or-transform-id" ]
bootstrap_servers = "10.14.22.123:9092,10.14.23.332:9092"
key_field = "user_id"
topic = "topic-1234"
compression = "none"
```


- **Elasticsearch**

```shell
[sinks.my_sink_id]
type = "elasticsearch"
inputs = [ "my-source-or-transform-id" ]
endpoint = "http://10.24.32.122:9000"
mode = "bulk"
pipeline = "pipeline-name"
compression = "none"
```

#### Global options reference

`enrichment_tables`：可以从外部读取数据源，为了使扩展表中的查找尽可能高效，数据将根据搜索中使用的字段进行索引。请注意，索引只能为一个具体匹配使用的提交字段创建。对于范围查询，一个索引不能被使用，并且扩展表回退到数据的顺序扫描。


```shell
# set global options
[enrichment_tables.csv]
    type = "file"
    file.path = "./info2.csv"
    file.encoding.type = "csv"

# [sources.generate_syslog]
# type = "stdin"

[sources.kafka_log]
type = "kafka"
bootstrap_servers = "172.18.2.39:9092"
group_id = "consumer-group-name"
topics = ["test2"]
auto_offset_reset= "latest"

[transforms.remap_syslog]
inputs = ["kafka_log"]
type = "remap"

source = '''
    .test, err = get_enrichment_table_record("csv", { "id": "1" })
    if is_null(err) {
        log("***********")
    }
'''

[sinks.emit_syslog]
inputs = ["remap_syslog"]
type = "console"
encoding.codec = "json"

```

csv格式配置文件,注意添加字段`field`

```
id,domain,value
1,abc.com,H
2,xxx,G
3,myworld,Hello
4,t00img.yangkeduo.com,1
```

### 运行命令

```
vector --config ./vector.toml
```
