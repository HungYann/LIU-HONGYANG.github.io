---

layout: post
author: LIU,HONGYANG
tags: [Hadoop]
---



1.数据库的底层是一张表，数据库的作用是对表进行读写。



2.使用软连接配置环境变量，因为比较方便直接修改软件，不用修改位置



### 配置Hbase步骤



1.上传

2.解压

3.重命名

4.修改环境变量

```shell
cat /etc/profile
```



5.把hadoop的core-site.xml和hdfs-site.xml分发到其它机器上

```shell
scp -r /hbase hadoop1:/hbase
```

6.启动

7.查看



### 逻辑存储结构







##### 表存储

![image-20200524193258768](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf3rplq53cj30zh0u0ar5.jpg)



RowKey: 关系型数据库中的主键，按照字典序排序，最大长度为64K. 

Column Family: 列簇，必须在创建表的时候就指定好，官方推荐的列簇数量为1～3个

列：属于某一个列簇，在Hbase中可以进行动态添加。

Cell：指具体的Value

Version: 在这张图中未显示出来，这个是指版本号，用时间戳（TimeStamp）





##### K-V组成

![image-20200524193519807](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf3rryicn3j30vy0c0t9r.jpg)



`KEY` 的组成是以 `Row key` 、`CF(Column Family)` 、`Column` 和 `TimeStamp` 组成的。

`TimeStamp` 在 `HBase` 中充当的作用就是版本号，因为在 `HBase` 中有着数据多版本的特性，所以同一个 `KEY` 可以有多个版本的 `Value` 值（可以通过配置来设置多少个版本）。查询的话是默认取回最新版本的那条数据，但是也可以进行查询多个版本号的数据.




### 命令行操作



在Hbase中创建：



![image-20200524220856486](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf3w7t7fizj315c0gktgf.jpg)



使用put命令，插入数据:



```shell
put 'emp','1','personal data:name','ruju'
put 'emp','1','personal data:city','hyderabad'
put 'emp','1','professional data:designation','manager'
put 'emp','1','professional data:salary','salary'
```





##### 1.导入数据



```
hadoop fs -put cars.csv /
```



![Screenshot 2020-05-25 at 14.05.53](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf4nw618zoj30v60ewjsp.jpg)

‘

##### 2.创建表



```
create 'cars','info'
```



![Screenshot 2020-05-25 at 14.08.44](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf4nym2koij30k202ajrj.jpg)





##### 3.将hdfs上的/cars.csv导入hbase表中cars表中



格式：hbase [类] [分隔符] [行键，列族] [表] [导入文件]



```shell
hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator="," -Dimporttsv.columns=HBASE_ROW_KEY,info:user_id,info:item_id,info:behavior_type,info:behavior_type,info:time cars /cars.csv
```



```shell
hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator="," -Dimporttsv.columns=HBASE_ROW_KEY,info:Car,info:MPG,info:Cylinders,info:Displacement,info:Horsepower,info:Weight,info:Acceleration,info:Model,info:Origin cars /cars.csv
```





info是列族，后边的user_id,item_id...是small_user_hbase.csv中的头部信息，导入后就是info列族中的列



```
hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv  -Dimporttsv.separator="," -Dimporttsv.columns=HBASE_ROW_KEY,cf:name test /data/test.csv
```









#####  插入数据

```shell
put 'student','1001','info:sex','male'
```





##### 扫描

```
scan 'student'
```





##### 查看表结构

```
describe 'student'
```





##### 获取表数据

```
get 'student','1001'
```





##### 删除表数据

```
delete 'student','1001','info:name'
deleteall 'student','1002'
```





##### 表的个数

```
count 'student'
```



##### 清空表

```
truncate 'student'
```







### References:



HBase创建数据:https://www.cnblogs.com/xinfang520/p/7717399.html



APACHE HBASE SHELL COMMANDS：https://www.corejavaguru.com/bigdata/hbase-tutorial/shell-commands



一文讲清HBase存储结构： https://juejin.im/post/5c31cf486fb9a04a102f6f89

