---
layout: post
author: LIU,HONGYANG
tags: [Flume]
---

利用flume和vector实现日志传输的架构：

flume涉及的总体架构：
 ![](https://tva1.sinaimg.cn/large/e6c9d24ely1h0cqp5q9e8j20jn05v3yk.jpg)


### confd组件

执行命令，更新confd内容，
```shell
/opt/confd/bin/confd -onetime --backend=etcdv3 --confdir=/etc/confd -scheme=https
```

配置文件位于`/etc/confd/`路径下，

文件路径如下图所示：

![](https://tva1.sinaimg.cn/large/e6c9d24ely1h09kvnxrsfj20j60bo3zo.jpg)

文件更新后，查看利用confd生成的新文件（ps:这里利用confd拉取etcd中存储的key值），为了简化，这里不涉及具体内容。

通过查看`parent-log-white.tooml`文件，
```shell
[template]
src = "parent-log-white.toml.tmpl"
dest = "/opt/vector/parent.toml"
keys = [
	"/test"
]

```

confd生成`/opt/vector/parent.toml`，vector运行

```shell
vector --config /opt/vector/parent.toml
```

# vector组件

vector可以通过配置`http`的方式，向flume传输数据，
vector用http的sink，flume用http的source收。

```shell
[sinks.my_sink_id]
type = "http"
inputs = [ "remap_syslog" ]
uri = "http://localhost:9001/"
compression = "none"
encoding = "json"

```

# flume配置

在从kafka发送数据，flume接收时，出现如下错误，
![](https://tva1.sinaimg.cn/large/e6c9d24ely1h0ctll0gewj22b2046acf.jpg)

根本原因是9001端口被占用，更换端口后，flume正常写入。

排查方式是通过curl手动发送命令信息[curl](https://blog.csdn.net/m0_37886429/article/details/104399554),

```shell  
curl -X POST -H 'Content-Type: application/json; charset=UTF-8' -d '{"username":"xyz","password":"123"}' http://yourdomain.com:81/
```

```shell
./bin/flume-ng agent -n a1 -f conf/flume-http.conf -Dflume.root.logger=INFO.console
```

```
2022-03-16 14:44:47,725 ERROR lifecycle.LifecycleSupervisor: Unable to start EventDrivenSourceRunner: { source:Avro source r1: { bindAddress: localhost, port: 9000 } } - Exception follows.
org.jboss.netty.channel.ChannelException: Failed to bind to: localhost/127.0.0.1:9000
	at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:297)
	at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
	at org.apache.flume.source.AvroSource.start(AvroSource.java:236)
	at org.apache.flume.source.EventDrivenSourceRunner.start(EventDrivenSourceRunner.java:44)
	at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:251)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.BindException: Address already in use
```
可以看到上诉报错命令


flume配置项目：

```shell   
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = http
a1.sources.r1.port = 9001
a1.sources.r1.channels = c1

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://0.0.0.0:9000/flume/%y-%m-%d/%H%M/%S
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = second
a1.sinks.k1.hdfs.useLocalTimeStamp = true
a1.sinks.k1.hdfs.rollInterval = 30

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

配置完成后，使用flume命令运行，


```shell  
./bin/flume-ng agent -n a1 -c conf/ -f job/flume-hdfs.conf -Dflume.root.logger=INFO,console
```

在写入hdfs时出现报错，在排查时发现，错误原因是flume中一个包的版本过低，需要从hadoop common目录下拷贝,

[错误解决方案](https://blog.csdn.net/weixin_47303949/article/details/108005162)

一开始采用了这种方案，但是发现，依然不行，通过更换flume版本，从1.7升级为1.9即可彻底解决。

新版下载地址[flume](https://www.apache.org/dyn/closer.lua/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz)



```
2022-03-16 18:54:43,513 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.hdfs.HDFSSequenceFile.configure(HDFSSequenceFile.java:63)] writeFormat = Writable, UseRawLocalFileSystem = false
2022-03-16 18:54:43,564 (SinkRunner-PollingRunner-DefaultSinkProcessor) [ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:459)] process failed
java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)
	at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1679)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:209)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:514)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:418)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:750)
Exception in thread "SinkRunner-PollingRunner-DefaultSinkProcessor" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)
	at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1679)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:209)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:514)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:418)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:750)
```

排查过程中，采用了flume-hdfs.conf文件，通过netcat发送命令，便于排查，参考文件在附录中

![](https://tva1.sinaimg.cn/large/e6c9d24ely1h0cuyoexfaj21am05q0tr.jpg)



# 额外知识点

获取hdfs链接和端口号

```shell
hdfs getconf -confKey fs.default.name
```


# 附录

`flume-avro.conf`


```shell
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = avro
a1.sources.r1.channels = c1
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 9000


# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1


```


`flume-hdfs.conf`

```shell
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type=hdfs
a1.sinks.k1.hdfs.path=hdfs://0.0.0.0:9000/source/logs/%{type}/%Y%m%d
a1.sinks.k1.hdfs.filePrefix=events
a1.sinks.k1.hdfs.fileType=DataStream
a1.sinks.k1.hdfs.writeFormat=Text
a1.sinks.k1.hdfs.useLocalTimeStamp=true

a1.sinks.k1.hdfs.useLocalTimeStame=true
a1.sinks.k1.hdfs.rollCount=0
a1.sinks.k1.hdfs.rollInterval=30
a1.sinks.k1.hdfs.rollSize=10485760
a1.sinks.k1.hdfs.batchSize=20
a1.sinks.k1.hdfs.threadsPoolSize=10
a1.sinks.k1.hdfs.callTimeout=30000

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1


````

`flume-http.conf`

```shell
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = http
a1.sources.r1.port = 9001
a1.sources.r1.channels = c1
#a1.sources.r1.HttpConfiguration.sendServerVersion = false
#a1.sources.r1.ServerConnector.idleTimeout = 300

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://0.0.0.0:9000/flume/%y-%m-%d/%H%M/%S
a1.sinks.k1.hdfs.filePrefix = events-
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = second
a1.sinks.k1.hdfs.useLocalTimeStamp = true
a1.sinks.k1.hdfs.rollInterval = 30

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1


```

`flume-kafka.conf`

```shell
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = avro
a1.sources.r1.bind = localhost
a1.sources.r1.port = 5002

# Describe the sink
a1.sinks.k1.channel = c1
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.topic = mytopic
a1.sinks.k1.kafka.bootstrap.servers = localhost:9092
a1.sinks.k1.kafka.flumeBatchSize = 20
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1
a1.sinks.k1.kafka.producer.compression.type = snappy


# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

```

`flume-telnet-logger.conf`
```shell
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.package07.CustomInterceptor$Builder
a1.sources.r1.selector.type = com.package06.CustomChannelSelector
a1.sources.r1.selector.header = domainTopic
a1.sources.r1.selector.isms = c1
a1.sources.r1.selector.parent = c1
a1.sources.r1.selector.default = c1
a1.sources.r1.selector.channels = c1

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1


```


`flume-env.sh`


```shell  
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# If this file is placed at FLUME_CONF_DIR/flume-env.sh, it will be sourced
# during Flume startup.

# Enviroment variables can be set here.

export JAVA_HOME=/etc/alternatives/java_sdk_1.8.0
export PATH=$JAVA_HOME/bin:$PATH


# export JAVA_HOME=/usr/lib/jvm/java-6-sun

# Give Flume more memory and pre-allocate, enable remote monitoring via JMX
# export JAVA_OPTS="-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote"

# Note that the Flume conf directory is always included in the classpath.
#FLUME_CLASSPATH=""


```
