---
layout: post
author: [LIU,HONGYANG]
tags: [Flume]
---

学习视频：


大数据Flume教程丨大数据业务及数据采集和迁移需求(https://www.bilibili.com/video/BV1tV411U7mu?t=33)


### 案例需求

监听数据端口案例分析:

1.通过netcat工具想本机的44444端口发送数据

2.Flume监控本机的44444端口。通过Flume的source端读取数据

3.Flume将获取的数据通过Sink端写出到控制台上


![](https://tva1.sinaimg.cn/large/008i3skNly1gr7e2col7zj30ex06074n.jpg)

Flume作用：传输log data, 日志数据        " for insert mode"

首先应该下载`netcat`, 

开启服务端口

```
nc -lk 44444
```

开启客户端

```
nc 44444
```


查看端口是否被占用

```
netstat -tlnp | greo 44444
```

创建job

```
mkdir job
cd job
touch netcat-flume-logger.conf
```

其中`netcat-flume-logger.conf` 监控端口数据

[文件内容](https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html)



```
# example.conf: A single-node Flume configuration

# Name the components on this agent
# 代表agent中可以有多个组件
# agent的名称需要唯一
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
# 监听端口号
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
# 定义类型
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
# 缓冲大小(events事件)
a1.channels.c1.capacity = 1000
# 事物相关容量
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
source和sink是1对n的关系
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

```

执行如下命令




```
bin/flume-ng agent --conf conf --conf-file job/netcat-flume-logger.conf --name a1 -Dflume.root.logger=INFO,console
```


```
bin/flume-ng agent -n a1 -c conf -f job/netcat-flume-logger.conf -Dflume.root.logger=INFO,console
```





日志信息:

```
Info: Sourcing environment configuration script /opt/apache-flume-1.9.0-bin/conf/flume-env.sh
Info: Including Hive libraries found via () for Hive access
+ exec /etc/alternatives/java_sdk/bin/java -Xmx20m -Dflume.root.logger=INFO,console -cp '/opt/apache-flume-1.9.0-bin/conf:/opt/apache-flume-1.9.0-bin/lib/*:/lib/*' -Djava.library.path= org.apache.flume.node.Application --conf-file job/netcat-flume-logger.conf --name a1
2021-06-07 07:18:45,509 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start(PollingPropertiesFileConfigurationProvider.java:62)] Configuration provider starting
2021-06-07 07:18:45,518 (conf-file-poller-0) [INFO - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWtcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:138)] Reloading configuration file:job/netcat-flume-logger.conf
2021-06-07 07:18:45,529 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig(FlumeConfiguration.java:1203)] Processing:c1
2021-06-07 07:18:45,530 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig(FlumeConfiguration.java:1203)] Processing:c1
2021-06-07 07:18:45,530 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig(FlumeConfiguration.java:1203)] Processing:r1
2021-06-07 07:18:45,530 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig(FlumeConfiguration.java:1203)] Processing:r1
2021-06-07 07:18:45,530 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig(FlumeConfiguration.java:1203)] Processing:r1
2021-06-07 07:18:45,530 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1117)] Added sinks: k1 Agent: a1
2021-06-07 07:18:45,531 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig(FlumeConfiguration.java:1203)] Processing:c1
2021-06-07 07:18:45,531 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig(FlumeConfiguration.java:1203)] Processing:k1
2021-06-07 07:18:45,531 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig(FlumeConfiguration.java:1203)] Processing:r1
2021-06-07 07:18:45,531 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig(FlumeConfiguration.java:1203)] Processing:k1
2021-06-07 07:18:45,531 (conf-file-poller-0) [WARN - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet(FlumeConfiguration.java:623)] Agent configuration for 'a1' has no configfilters.
2021-06-07 07:18:45,549 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration.validateConfiguration(FlumeConfiguration.java:163)] Post-validation flume configuration contains configuration for agents: [a1]
2021-06-07 07:18:45,549 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:151)] Creating channels
2021-06-07 07:18:45,556 (conf-file-poller-0) [INFO - org.apache.flume.channel.DefaultChannelFactory.create(DefaultChannelFactory.java:42)] Creating instance of channel c1 type memory
2021-06-07 07:18:45,561 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:205)] Created channel c1
2021-06-07 07:18:45,562 (conf-file-poller-0) [INFO - org.apache.flume.source.DefaultSourceFactory.create(DefaultSourceFactory.java:41)] Creating instance of source r1, type avro
2021-06-07 07:18:45,574 (conf-file-poller-0) [INFO - org.apache.flume.sink.DefaultSinkFactory.create(DefaultSinkFactory.java:42)] Creating instance of sink: k1, type: logger
2021-06-07 07:18:45,577 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:120)] Channel c1 connected to [r1, k1]
2021-06-07 07:18:45,583 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:162)] Starting new configuration:{ sourceRunners:{r1=EventDrivenSourceRunner: { source:Avro source r1: { bindAddress: localhost, port: 44444 } }} sinkRunners:{k1=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@1bd188b7 counterGroup:{ name:null counters:{} } }} channels:{c1=org.apache.flume.channel.MemoryChannel{name: c1}} }
2021-06-07 07:18:45,588 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:169)] Starting Channel c1
2021-06-07 07:18:45,744 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:119)] Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.
2021-06-07 07:18:45,744 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:95)] Component type: CHANNEL, name: c1 started
2021-06-07 07:18:45,745 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:196)] Starting Sink k1
2021-06-07 07:18:45,747 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:207)] Starting Source r1
2021-06-07 07:18:45,747 (lifecycleSupervisor-1-4) [INFO - org.apache.flume.source.AvroSource.start(AvroSource.java:193)] Starting Avro source r1: { bindAddress: localhost, port: 44444 }...
2021-06-07 07:18:46,021 (lifecycleSupervisor-1-4) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:119)] Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.
2021-06-07 07:18:46,022 (lifecycleSupervisor-1-4) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:95)] Component type: SOURCE, name: r1 started
2021-06-07 07:18:46,026 (lifecycleSupervisor-1-4) [INFO - org.apache.flume.source.AvroSource.start(AvroSource.java:219)] Avro source r1 started.a

```

======================


**组件** 


- Source: 采集源，用于跟数据源对接，以获取数据

- channel:缓存

- Sink: 下沉地，采集数据的传送目的，用于往下一级agent传递数据或往最终系统


**Flume采集系统分为两种：**

1.单个结构

2.复杂结构（多级agent串联）


**运行机制**

flume本身是java程序，在需要采集数据机器上启动-> agent进程

agent进程包含： source, sink, channel

在flume中，数据被包装成event，真实的数据是放在event body中


##### 安装部署

- 1. 进入flume目录，修改flume-env.sh，配置JAVA_HOME路径

- 2. 根据数据采集需求配置数据采集方案

- 3. 指定采集方案配置文件，启动flume agent


为flume添加运行权限
```
chmod a+x flume-env.sh
```


##### 开发步骤

文件名，通常以source-sink.conf命名

再通过linux flume agent方式运行




##### 功能总结

- 监控文件夹变化

当文件夹中添加新数据时，不断将文件中内容转移到sink中

需要注意点：1.文件不能同名，否则会停止执行
            2.一般解决方式是加时间戳


- 监控文件变化

当文件中有新添加的内容时，`tail -f xx.log`

- flume串联跨网络传输数据


flume一个进程无法满足要求时，需要多个组件来完成任务

avro sink

avro source

使用上诉两个组件指定绑定的端口ip，就可以满足数据跨网络的传递，通常用于flume串联架构中

![](https://tva1.sinaimg.cn/large/008i3skNly1grbu8aj5wmj31360k47c0.jpg)


```
#set sink1

agent1.sinks.k1.channel=c1
agent1.sinks.k1.type=avro
agent1.sinks.k1.hostname=node-2
agent1.sinks.k1.port=52020

# set sink2

agent1.sinks.k2.channel=c1
agent1.sinks.k2.type=avro
agent1.sinks.k2.hostname=node-3
agent1.sinks.k2.port=52020

# set sink group

agent1.sinkgroups.g1.sinks=k1 k2


# set failover

agent1.sinkgroups.g1.processor.type=load_balance
agent1.sinkgroups.g1.processor.backoff=true
agent1.sinkgroups.g1.processor.selector=round_robin #轮训
agent1.sinkgroups.g1.processor.maxTimeOut=10000
```

在设置avro的过程中,Agent1需要配置k2.hostname和port

Agent2和Agent3分别监听本机，不能跨机器(也就是说，在不同机器上分别配置)


flume串联启动流程：从远离数据源的位置开始启动，因为没有数据

启动命名：

```
bin/flume-ng flume agent -c conf -f conf/exec-avro.conf -n agent1 -Dflume.root.logger=INFO,console
```


##### Failover sink processor(容错)

问题？如何解决单点故障

**设置备份**


设置在agent1中, 第2和第3不变
```
#set failover

agent1.sinkgroups.g1.processor.type=failover
agent1.sinkgroups.g1.processor.priority.k1=10
agent1.sinkgroups.g1.processor.priority.k2=1
agent1.sinkgroups.g1.processor.maxpenalty=10000
```


##### 设置拦截器


在flume中，数据都是以event形式流转，需要定义拦截器进行区分

日志采集和汇总：A、B两台日志服务器实时生产日志主要类型为access.log、ngix.log、web.log

**现在要求**：把A、B机器中的access.log、nigx.log、web.log采集汇总到C机器上然后统一收集到hdfs中。

但是在hdfs中要求的目录为：

```
/source/logs/access/20160101/**
/source/logs/nginx/20160101/**
/source/logs/web/20160101/**
```


- 静态拦截器

exec_source_avro_sink.conf

第一级配置：

```
#Name the components on this agent
a1.sources=r1 r2 r3
a1.sinks=k1
a1.channels=c1

#Describe/configure the soruce
a1.sources.r1.type=exec
a1.sources.r1.command=tail -F /home/hadoop/logs/access.log
a1.sources.r1.interceptors=i1
a1.sources.r1.interceptors.i1.type=static
a1.sources.r1.interceptors.i1.key=type
a1.sources.r1.interceptors.i1.value=access

a1.sources.r2.type=exec
a1.sources.r2.command=tail -F /home/hadoop/logs/nginx.log
a1.sources.r2.interceptors=i2
a1.sources.r2.interceptors.i2.type=static
a1.sources.r2.interceptors.i2.key=type
a1.sources.r2.interceptors.i2.value=nginx

a1.sources.r3.type=exec
a1.sources.r3.command=tail -F /home/hadoop/logs/web.log
a1.sources.r3.interceptors=i3
a1.sources.r3.interceptors.i3.type=static
a1.sources.r3.interceptors.i3.key=type
a1.sources.r3.interceptors.i3.value=web

#Describe the sink


a1.sinks.k1.type=avro
a1.sinks.k1.hostname=hdp-node-03
a1.sinke.k1.port=41414


# Use a channel which buffers events in memory
a1.channels.c1.type=memory
a1.channels.c1.capacity=2000
a1.channels.c1.transactionCapacity=100

#Bind the source and sink to the channel
a1.sources.r1.channels=c1
a1.sources.r2.channels=c1
a1.sources.r3.channels=c1
a1.sinks.k1.channel=c1
```


第二级配置：


avro_source_hdfs_sink.conf

```
#定义agent名，source、channel、sink的名称
a1.sources=r1
a1.sink=k1
a1.channels=c1

#定义source
a1.sources.r1.type=avro
a1.sources.r1.bind=node-2
a1.sources.r1.port=41414

#添加时间拦截器
a1.sources.r1.interceptors=i1
a1.sources.r1.interceptros.i1.type=org.apache.flume.interceptor.TimestampInterceptor$Builder

#定义channels

a1.channels.c1.type=memory
a1.channels.c1.capacity=20000
a1.channels.c1.transactionCapacity=10000

#定义sink
a1.sinks.k1.type=hdfs
a1.sinks.k1.hdfs.path=hdfs://node-1:9000/source/logs/%{type}/%Y%m%d
a1.sinks.k1.hdfs.filePrefix=events
a1.sinks.k1.hdfs.fileType=DataStream
a1.sinks.k1.hdfs.writeFormat=Text


#时间类型 
a1.sinks.k1.hdfs.useLocalTimeStame=true
#生成的文件按条数生成
a1.sinks.k1.hdfs.rollCount=0
#生成的文件按时间生成
a1.sinks.k1.hdfs.rollInterval=30
#生成的文件按大小生成(字节)
a1.sinks.k1.hdfs.rollSize=10485760
#批量写入hdfs的个数
a1.sinks.k1.hdfs.batchSize=20
#flume操作hdfs的线程数
a1.sinks.k1.hdfs.threadsPoolSize=10
#操作hdfs超时time
a1.sinks.k1.hdfs.callTimeout=30000

#组装source、channel、sink
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1


```

模拟数据实时产生



```
while true; do echo "access">> /root/logs/access.log;sleep 0.5;done

while true; do echo "nginx">> /root/logs/nginx.log;sleep 0.5;done


while true; do echo " ">> /root/logs/access.log;sleep 0.5;done
```


先启动第二级命令，再启动第一级


##### 自定义拦截器

Flume自带拦截器，比如：TimestampInterceptor、HostInterceptor、RegexExtractorInterceptor等，通过使用不同的拦截器，实现不同的功能


目的：自定义拦截器，过滤不需要的字段，并对指定字段加密处理，将源数据进行预处理。flume还有拦截的功能


![](https://tva1.sinaimg.cn/large/008i3skNly1grbzzbh8etj311c0hs4bx.jpg)

拦截器的位置：位于`source`下面

修改flume的配置信息

spool-intercetpr-hdfs.conf

```
a1.channels=c1
a1.sources=r1
a1.sinks=s1

#channel
a1.channels.c1.type=memory
a1.channels.c1.capacity=100000
a1.channels.c1.transactionCapacity=50000

#source(重点)

a1.sources.r1.channels=c1
a1.sources.r1.type=spooldir
a1.sources.r1.spoolDir=/root/data
a1.sources.r1.batchSize=50
a1.sources.r1.inputCharset=UTF-8


a1.sources.r1.interceptors=i1 i2
a1.sources.r1.interceptors.i1.type=cn.itcast.interceptor.CustomParameterInterceptor$Builder
a1.sources.r1.interceptors.i1.fields_separateor=\\u0009
a1.sources.r1.interceptors.i1.indexs=0,1,3,5,6
a1.sources.r1.interceptors.i1.indexs_separator=\\u002c
a1.sources.r1.interceptors.i1.encrypted_field_index=0

a1.sources.r1.interceptors.i2.type=
org.apache.flume.interceptor.TimestampInterceptor$Builder


#sink 


a1.sinks.s1.channel=c1
a1.sinks.s1.type=hdfs
a1.sinks.s1.hdfs.path=hdfs://node-1:9000/intercept/%Y%m%d
a1.sinks.s1.hdfs.filePrefix=itcasr
a1.sinks.s1.hdfs.fileSuffix=.dat
a1.sinks.s1.hdfs.rollSize=10485760
a1.sinks.s1.hdfs.rollInterval=20
a1.sinks.s1.hdfs.rollCount=0
a1.sinks.s1.hdfs.batchSize=2
a1.sinks.s1.hdfs.round=2
a1.sinks.s1.hdfs.roundUnit=minute
a1.sinks.s1.hdfs.threadsPoolSize=25
a1.sinks.s1.hdfs.useLocalTimeStamp=true
a1.sinks.s1.hdfs.minBlockReplicas=1
a1.sinks.s1.hdfs.fileType=DataStream
a1.sinks.s1.hdfs.writeFormat=Text
a1.sinks.s1.hdfs.callTimeout=60000
a1.sinks.s1.hdfs.idleTimeout=60
```


公司使用的是tyrande，

```
//通过flume context上下文环境变量读取采集方案中配置的属性，如果没有配置又默认实现

public class CustomParameterInterceptor implements Interceptor{
    

    // 3.通过CustomParameterInterceptor的构造器获取到配置文件中的参数
    public CustomParameterInterceptor(String fields_separator,
        String indexs, String index_separator, String encrypted_field_index
            ){
        //nothing to do
    }

    
    //最终，调用该方法
    public Event intercept(Event event){
    
    }

    //1.步骤一,获取
    public static class Builder implements Interceptor.Builder{
        public void configure(Context context){

    }
    //2.构造
    public Interceptor build(){
        new ....//将参数传递给外部类
    }
}



```

最后将打好的tar包，放在flume的lib目录下



##### 自定义source


[参考文档](https://flume.apache.org/releases/content/1.9.0/FlumeDeveloperGuide.html#source)



```
public class MySource extends AbstractSource implements Configurable, PollableSource {
  private String myProp;

  @Override
  public void configure(Context context) {
    String myProp = context.getString("myProp", "defaultValue");

    // Process the myProp value (e.g. validation, convert to another type, ...)

    // Store myProp for later retrieval by process() method
    this.myProp = myProp;
  }

  @Override
  public void start() {
    // Initialize the connection to the external client
  }

  @Override
  public void stop () {
    // Disconnect from external client and do any additional cleanup
    // (e.g. releasing resources or nulling-out field values) ..
  }

  @Override
  public Status process() throws EventDeliveryException {
    Status status = null;

    try {
      // This try clause includes whatever Channel/Event operations you want to do

      // Receive new data
      Event e = getSomeData();

      // Store the Event into this Source's associated Channel(s)
      getChannelProcessor().processEvent(e);

      status = Status.READY;
    } catch (Throwable t) {
      // Log exception, handle individual exceptions as needed

      status = Status.BACKOFF;

      // re-throw all Errors
      if (t instanceof Error) {
        throw (Error)t;
      }
    } finally {
      txn.close();
    }
    return status;
  }
}

```



##### 自定义sink


```
sink需要实现Configurable

```

开启日志

```
bin/flume-ng agent -c conf -f conf/filesink.conf -n a1 -Dflume.root.logger=INFO,console
```



##### 待学。。。







补充：Flume教程(flume框架快速入门)（https://www.bilibili.com/video/BV1cK4y177bb?t=20&p=5）

