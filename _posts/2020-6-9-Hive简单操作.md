---
layout: post
author: LIU,HONGYANG
tags: [Hadoop]
---





Author: LIU,HONGYANG

Matrix Number: 17201091/1





### Before the Lab Test:



##### Start the Hadoop and check the daemons:



```shell
start-all.sh
jps
```

![Screenshot 2020-06-09 at 17.45.03](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm6ibrnfzj30k80d0777.jpg)



##### DownLoad dabasets:

![Screenshot 2020-06-09 at 18.32.21](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm7vhq5ugj30t0014t8q.jpg)





### LabTest:



##### Part 1:



1.Import the downloaded dataset to HDFS



code:

```
hdfs dfs -put ~/Desktop/Set5.csv /user/Set5.csv
```



![image-20200609183932620](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm82us13yj30k2012t8v.jpg)



Results:



![Screenshot 2020-06-09 at 18.39.16](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm832udzoj30vi0hq0us.jpg)





2.By using Hive or Pig, identify 5 rows of data that have the 



1. highest reading score.
2. lowest CGPA.



Create database:



![image-20200609193842851](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm9sexnorj30ce03y74k.jpg)



Create table:



```
create table labtest(No int, gender string, race string, education string, lunch string, course string, math int, reading int, writing int) row format delimited fields terminated by ',';
```



![Screenshot 2020-06-09 at 18.43.54](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm87go2cvj30xe04aae1.jpg)



```
desc labtest;
```





![image-20200609184915297](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm8cy5chvj30c2062glx.jpg)







load data from hdfs to hive:



```
load data inpath '/user/Set5.csv' overwrite into table labtest;
```



![image-20200609185359067](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm8hvprzkj315404swm1.jpg)





![Screenshot 2020-06-09 at 18.51.39](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm8ikxm4rj30qg06ktad.jpg)





###### Answer 1:



**highest reading score(5 rows):**



```
select reading from labtest order by reading desc limit 5;
```



![image-20200609185747823](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm8lumci5j30wq0ek77w.jpg)



They are 100, 99, 93, 92, 90





###### Answer 2:



**lowest writing score(5 rows):**



```
select writing from labtest order by writing asc limit 5;
```



![image-20200609190249761](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm8r2m58wj30w80eigp1.jpg)



They are 34, 38, 42,43,45





##### Part 2:



download file and upload it to hdfs



```
wget http://www.gutenberg.org/files/12345/12345-8.txt
hdfs dfs -put ~/Desktop/12345-8.txt /user/12345-8.txt
```

![image-20200609191445794](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm93i0v9hj30rq07itaq.jpg)



###### Answer 1:

![image-20200609191416803](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm92ziefpj30ve0hm0uq.jpg)





##### Answer 2:



enter the file example file: hadoop-mapreduce-examples-2.7.7.jar



![image-20200609192149280](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm9augssjj30wm02e3z6.jpg)



run shell:

```
hadoop jar hadoop-mapreduce-examples-2.7.7.jar wordcount /user/12345-8.txt /user/results
```



check results:

```
hdfs dfs -cat /user/results/part-r-0000
```



![image-20200609192857958](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm9i9opigj317w0hmwlb.jpg)





##### Answer 3:











```
create table wordcount(word string, number int) row format delimited fields terminated by '\t';
```



![image-20200609205036374](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfmbv7uwvcj30r409kwfm.jpg)





```
desc wordcount;
```



![image-20200609210051475](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfmc6htk93j30bk02i74c.jpg)





> a

```
select *  from wordcount order by number desc, word asc limit 5;
```



![image-20200609211546279](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfmclehv1jj30v40emtc7.jpg)





> b

```
 select * from wordcount where number=5 order by word desc limit 5;
```



![image-20200609210350124](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfmc8zfgihj30uy0eg0w9.jpg)

