---
layout: post
author: LIU,HONGYANG
tags: [Hadoop]
---





### Hadoop介绍



海量数据面临的问题：

> 1.寻址速度小于传输速度：

更新比较麻烦

![image-20200408221857773](https://tva1.sinaimg.cn/large/00831rSTgy1gdmq0393nej30xo07utcv.jpg)



关系型数据库和Mapreduce的比较：

![image-20200411165616950](https://tva1.sinaimg.cn/large/007S8ZIlgy1gdpxj9ydduj30sy0c043f.jpg)



>  2.数据的类型



Hadoop 具备高速流读/写操作，非常适合于分析日志(非结构化数据)



##### Hadoop权威指南目录



![image-20200411181452207](https://tva1.sinaimg.cn/large/007S8ZIlgy1gdpzt07thij30lu0rmgz3.jpg)



第I部分：



| 第I部分       | **Hadoop基础知识**     |
| ------------- | ---------------------- |
|               | Hadoop宏观介绍         |
|               | 关于Mapreduce          |
|               | Hadoop分布式文件HDFS   |
|               | 关于Yarn               |
|               | Hadoop的I/O操作        |
| **第II部分**  | **关于MapReduce**      |
|               | Mapreduce应用开发      |
|               | Mapreduce的工作机制    |
|               | Mapreduce的类型格式    |
|               | Mapreduce的特性        |
| **第III部分** | **Hadoop的操作**       |
|               | 构建Hadoop集群         |
|               | 管理Hadoop             |
| **第IV部分**  | **Hadoop相关开源项目** |
|               | 关于Avro               |
|               | 关于Parquet            |
|               | 关于Flume              |
|               | 关于Sqoop              |
|               | 关于Pig                |
|               | 关于Hive               |
|               | 关于Crunch             |
|               | 关于Spark              |
|               | 关于Hbase              |
|               | 关于Zookeeper          |
| **第V部分**   | **案例**               |
|               | 医疗公司可聚合数据     |
|               | 生物数据科学           |
|               | 开源项目               |
|               |                        |



### 关于MapReduce

采用的数据集来自[美国国家气候数据中心](https://www.ncdc.noaa.gov/stormevents/choosedates.jsp?statefips=1%2CALABAMA), 数据以ASCII格式存储，其中每一行是一条记录。

输入数据类型：（省略未使用的列）

![image-20200411200223814](https://tva1.sinaimg.cn/large/007S8ZIlgy1gdq2wvuw1nj30xo05mjux.jpg)



这些行以键-值对的方式作为map函数的输入:



![image-20200411200303767](https://tva1.sinaimg.cn/large/007S8ZIlgy1gdq2xkls11j30xg05m0wq.jpg)

map函数功能仅限于提取年份和气温信息。

(1950,0) (1949,78)

reduce函数处理基于键来对键值进行排序，分组

(1949,[111,78])

(1950,[0,22,-11])



处理流程如下：

![Screenshot 2020-04-11 at 20.05.43](https://tva1.sinaimg.cn/large/007S8ZIlgy1gdq318munpj30xo08mdm1.jpg)



使用Java来实现MapReduce代码：



Mapper类：

```java
package com.datapig.chapter02;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;

import java.io.IOException;

/**
 * description:
 * LongWriatable:相当于Java的Long类型
 * Text:相当于Java的String类型
 * IntWriable:相当于Java的Integer类型
 */
public class MaxTemperatureMapper extends MapReduceBase implements Mapper<LongWritable, Text,Text, IntWritable> {
    private static final int MISSING = 9999;
    @Override

    /**
     *OutputCollector是一个接口
     * TEXT对象：年份
     * IntWritable:气温
     */

    public void map(LongWritable longWritable, Text text, OutputCollector<Text, IntWritable> outputCollector, Reporter reporter) throws IOException {
        String line = text.toString();
        String year = line.substring(15,19);

        int airTemperature;

        if(line.charAt(87)=='+'){
            airTemperature = Integer.parseInt(line.substring(88,92));
        }
        else{
            airTemperature = Integer.parseInt(line.substring(87,92));
        }

        String quality = line.substring(92,93);
        if (airTemperature!=MISSING&&quality.matches("[01459]]")){
            outputCollector.collect(new Text(year), new IntWritable(airTemperature));
        }

    }
}

```

输入键是一个长整数偏移量，输入值是一行文本，输出键是年份，输出值是气温（整数）。



Reduce类：

```java
package com.datapig.chapter02;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;

import java.io.IOException;
import java.util.Iterator;

/*
TEXT:年份类型
IntWritable:最高气温
 */
public class MaxTemperatureReducer extends MapReduceBase implements Reducer<Text, IntWritable, Text,IntWritable> {


    @Override
    public void reduce(Text text, Iterator<IntWritable> iterator, OutputCollector<Text, IntWritable> outputCollector, Reporter reporter) throws IOException {
        int maxValue = Integer.MIN_VALUE;
        while (iterator.hasNext()){
            maxValue = Math.max(maxValue,iterator.next().get());
        }
        outputCollector.collect(text,new IntWritable(maxValue));
    }
}

```



MaxTemperature找到最高气温：



```java
package com.datapig.chapter02;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;

import java.io.FileOutputStream;
import java.io.IOException;

public class MaxTemperature {
    public static void main(String[] args) throws IOException {
        if(args.length!=2){

            System.err.println("Usage: MaxTemperature <input path> <output path>");
            //unsuccessful exit
            System.exit(-1);
        }

        JobConf conf = new JobConf(MaxTemperature.class);

        conf.setJobName("Max temperature");
        //输入的可以是单个文件、目录
        FileInputFormat.addInputPath(conf,new Path(args[0]));
        //reduce函数输出写入目录
        FileOutputFormat.setOutputPath(conf,new Path(args[1]));

        conf.setOutputKeyClass(Text.class);

        conf.setOutputValueClass(IntWritable.class);

        JobClient.runJob(conf);


    }
}

```



构造Job对象之后，需要指定输入和输出数据的路径。调用 FileInputFormat类的静态方法addInputPath()来定义输入数据的路径，这个路径可以是单个的文件、一个目录（此时，将目录下所有文件当作输入）或符合特定文件模式的一些列文件。



调用FileOutputFormat类的静态方法setOutputPath()来指定输出路径（只能有一个输出路径）。这个方法指定的是reduce函数输出文件的写入目录。**在运行作业前该目录是不应该存在的**,否则Hadoop会报错并拒绝运行作业。

注：可以防止数据丢失，不被意外覆盖



##### 数据流：

map 任务将其输出写入本地硬盘，而非HDFS。(map的输出为中间结果，该中间结果由reduece任务处理后才产生最终输出结果，一旦作业完成，map的输出结果就可删除)



### HDFS原理（Hadoop分布式文件系统）

管理网络中跨多台计算机存储的文件系统称为分布式文件系统，

hdfs中有三大组件NameNode、DataNode SecondaryNameNode

> 数据块

磁盘进行读/写的最小单位。HDFS块默认大小为128MB.(注：HDFS磁盘块大小和寻址时间有关)



优点：**

1.**一个文件的大小可以大于网络中任意一个磁盘的容量**，文件存储块不需要存储在一个磁盘上

2.**简化子系统设计**（块大小固定，可以确定磁盘可以容纳的块数）

3.提高容错能力



##### Java客户端访问HDFS程序：



接下来介绍：

如何使用Java来，

1.获取FileSystem

2.创建目录

3.创建文件

4.上传文件

5.查看目录元数据信息

6.下载文件

7.删除文件



使用get方法获取FileSystem

```java
  public static FileSystem getHadoopFileSystem(){

        Configuration conf = new Configuration();

        //指定主节点namenode访问地址
        conf.set("fs.defaultFS","hdfs://localhost:8020");
        conf.set("dfs.replication","1");
        try {
            //通过调用FileSystem工厂模式get方法生成文件系统
            FileSystem fileSystem = FileSystem.get(conf);
            return fileSystem;
        } catch (IOException e) {
            e.printStackTrace();
        }

        return null;

    }
```



使用`mkdirs`创建目录:

```java
public static boolean createPath(String pathName){
        boolean result = false;

        //1.获取文件系统
        FileSystem fileSystem = getHadoopFileSystem();
        Path path = new Path(pathName);


        try {
            //2.调用文件系统的mkdir目录
            result = fileSystem.mkdirs(path);
            //3.将文件系统关闭
            fileSystem.close();
        } catch (IOException e) {
            e.printStackTrace();
        }

        return result;

    }
```



创建文件并输入内容`content`:

```java
  public static boolean createFile(String pathName,String content){
        boolean result =false;
        //1.获取文件系统

        FileSystem fileSystem = getHadoopFileSystem();
        try {
            //2.调用文件系统create方法创建文件
            FSDataOutputStream out = fileSystem.create(new Path("/test",pathName));
            //or
//            fileSystem.create(new Path(pathName));
            //3.关闭文件系统
            out.writeUTF(content);

            fileSystem.close();
            result = true;
        } catch (IOException e) {
            e.printStackTrace();
        }


        return result;
    }
```





上传文件方法一`copyFromLocalFile`：

```java
public static boolean putFile(String srcPathName,String dstPathName){
        boolean result = false;
        //1.获取文件系统
        FileSystem fileSystem = getHadoopFileSystem();

        //2.调用文件系统中的方法
        try {
            fileSystem.copyFromLocalFile(new Path(srcPathName),new Path(dstPathName));

            fileSystem.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
        return result;
    }
```



上传文件方法二`create`：

```java
  public static boolean putFile2(String srcPathName,String dstPathName){
        boolean result = false;
        //1.获取文件系统
        FileSystem fileSystem = getHadoopFileSystem();

        //2.创建输出文件

        try {
            //通过上传文件生成输出流
            FSDataOutputStream out = fileSystem.create(new Path(dstPathName));
            //通过本地文件生成输入流
            FileInputStream in = new FileInputStream(srcPathName);

            //通过IOUtilsd的copyBytes方法传递数据流
            IOUtils.copyBytes(in,out,4096);
            result = true;

        } catch (IOException e) {
            e.printStackTrace();
        }
        return result;
    }
```



查看指定目录的文件的元数据信息`listStatus`：

```java
 public static void list(String pathName){
        //1.获取文件系统
        FileSystem fileSystem = getHadoopFileSystem();
        //2.调用文件系统listStatus方法获取元数据列表
        try {

            FileStatus[] list =  fileSystem.listStatus(new Path(pathName));
            for(FileStatus fileStatus:list){

                boolean isDir = fileStatus.isDirectory();
                String path = fileStatus.getPath().toString();
                //获取文件元数据信息
                short replication = fileStatus.getReplication();
                System.out.println(isDir+"::"+path+"::"+replication);
            }
            fileSystem.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
```



下载文件第一种方法：

```java
 public static void getFileFromHDFS(String hdfsSrcName,String dstPathName){
        //1.获取文件系统
        FileSystem fileSystem = getHadoopFileSystem();


        try {
            //2.调用文件系统的copyToLocalFile方法下载
            //copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem)
            //由于本地无hadoop环境，第四个参数指定使用本地文件系统
            fileSystem.copyToLocalFile(false,new Path(hdfsSrcName),new Path(dstPathName),true );

            fileSystem.close();

        } catch (IOException e) {
            e.printStackTrace();
        }

    }
```



下载文件的第二种方法`open`：

通过数据流的方法进行下载：

```java
public static boolean getFileFromHDFS2(String hdfsSrcPathName,String dstPathName){
            boolean result = false;
            //1.获取文件系统
            FileSystem fileSystem = getHadoopFileSystem();


        try {
            //通过输入输出流进行下载
            //2.1hdfs文件通过输入流读取到内存
            FSDataInputStream in = fileSystem.open(new Path(hdfsSrcPathName));
            //2.2内存中的数据通过输出流输出到本地文件中
            FileOutputStream out = new FileOutputStream(dstPathName);
            //2.3IOUtils方法复制数据流
            IOUtils.copyBytes(in,out,4096);
            //直接输出到控制台
            IOUtils.copyBytes(in,System.out,4096);
            result=true;
            fileSystem.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
        return result;

 
    }
```



删除文件`delete`:

```java
 public static void delete(String pathName){
        //1.获取文件系统
        FileSystem fileSystem = getHadoopFileSystem();
        //2.调用文件系统的删除方法

        try {
            fileSystem.delete(new Path(pathName),true);
            fileSystem.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
```



调用的主函数：

```java
 public static void main(String[] args) {
        //获取FileSystem
        //FileSystem fileSystem = getHadoopFileSystem();
        //System.out.println(fileSystem);

        //创建目录
        // HDFS API操作hdfs-site.xml文件,权限问题dfs.permissions.enables false
        //boolean result = createPath("/test");
        //System.out.println(result);

        //创建文件
//        boolean result = createFile("test.txt");
        //在根目录下创建一个文件
//        boolean result = createFile("/test/test.txt","hello world");
//        System.out.println(result);

        //上传文件
        //输出pathName可以是目录也可以是文件
//        putFile("localfileDirectory","/text/word.txt");
        //输出pathName必须是文件
//        putFile2("localfileDirectory/text.txt","/text/word1.txt");

        //获取元数据信息
        //list("/test");

        //下载文件
//        getFileFromHDFS("/test/test.txt","E://test");
        //第二个参数是文件，通过输入流，输出流，可以直接放到本地文件
//        getFileFromHDFS2("test/test/txt","E://word1");

        //删除文件
        delete("test");
    }
```





### 学习资源 ：



[阿里云大学](https://edu.aliyun.com/developer?spm=5176.8764728.aliyun-edu-course-rightpart.2.fc5612a7DWdlmH)

 [Java大数据学习之路](https://github.com/TALKDATA/JavaBigData?utm_source=wechat_session&utm_medium=social&utm_oi=1208923657256820736)



