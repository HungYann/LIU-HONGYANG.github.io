---
layout: post
author: LIU,HONGYANG
tags: [Python]
---



### Python爬虫使用BeautifulSoup



#### 爬虫

- 是请求网站并提取数据的自动化程序

- 用于数据采集，在数据，是data collection的一部分。

对于爬虫的学习，我建议爬取中国房价，并做成数据仓库中存储分析，构建大数据分析平台



##### demo 1



>  目标源代码：



```html
html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>

<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class="story">...</p>
"""
```



> 对数据进行标准格式化：

```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(html_doc)
print(soup.prettify())
```



>  获取相应的子标签：

```python
#浏览器结构化数据方法 soup对象+标签名
soup.title
soup.a
#甚至是在循环中出现
for child in soup.find('p',{'class','sister'}).children:
  print(child.a.get_text())
```



>  查找同名标签：



```python
#查找标签
soup.find_all('a')
#指定名称
soup.find_all('p',{'class','story'})
```



>  找到标签内容并打印：



```python
for link in soup.find_all('a'):
	print link['href']
```



>  从文档中获取所有文字：



```python
print(soup.get_text())
```



##### demo2



> 编写爬虫程序



```python
import requests

url = 'https://www.thestar.com.my/tech/tech-news/2020/03/02/get-in-gear-sweet-dreams-are-made-of-tech'

url_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'

headers = {'User-Agent':url_agent}

page = requests.get(url, headers=headers)

soup = BeautifulSoup(page.text,'lxml')

content = soup.find_all('div',{'class':'row content-holder story-wrapper'})
```



>  使用post请求



````python
import requests
import webbrowser
param = {'wd':'莫烦python'}
url = 'http://www.baidu.com/s'
r = requests.get(url,params=param)
print(r.url)
webbrowser.open(r.url)

data = {'firstname':'sdf','lastname':'sdf'}
r = requests.post('http://pythonscraping.com/pages/files/processing.php',data=data)
print(r.text)
````



##### demo 3:



> 使用selenium

```python
import selenium
from selenium import webdriver
driver = webdriver.Chrome(executable_path='/Users/liuhongyang/Desktop/chromedriver')


driver.get("https://www.thestar.com.my/business/marketwatch/")
driver.find_element_by_xpath("//button[@id='buttonsearch']/a/i").click()
driver.find_element_by_id("qTextBox").clear()
driver.find_element_by_id("qTextBox").send_keys("oil price")
driver.find_element_by_id("myform").submit()

html = driver.page_source
soup = BeautifulSoup(html,'html.parser')
content = soup.find_all('div',{'class':'container'})


```



>  使用pandas读取表格

```python
import pandas as pd
from bs4 import BeautifulSoup
import requests
url='https://www.indexmundi.com/commodities/?commodity=crude-oil&months=60'
r = requests.get(url)
df = pd.read_html(r.text)
df[1]
```



##### demo 4



> 爬取链家网站



```python
import requests

url = 'https://bj.lianjia.com/ershoufang/pg3/'

headers={
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'
}

resp = requests.get(url,headers=headers)


# print(resp.content.decode('utf-8'))  网页内容 二进制
# print(resp.text) 网络内容 文本

soup = BeautifulSoup(resp.text,'lxml')

infos = soup.find('ul',{'class','sellListContent'}).find_all('li')



for info in infos:
    name = info.find('div',{'class':'title'}).find('a').get_text()
    print(name)
    totalPrice = info.find('div',{'class':'priceInfo'}).find('div',{'class':'totalPrice'}).find('span').get_text()
    print(totalPrice)
    unitPrice = info.find('div',{'class':'priceInfo'}).find('div',{'class':'unitPrice'}).find('span').get_text()
    print(unitPrice)
    address = info.find('div',{'class','address'}).find('div',{'class','houseInfo'}).get_text()
    print(address)
    with open('lianjia.csv','a',encoding='utf-8') as f:
        f.write("{},{},{},{}\n".format(name,totalPrice,unitPrice,address))
```





>  使用selenium获取第二页



```python
import selenium
from selenium import webdriver
driver = webdriver.Chrome(executable_path='/Users/liuhongyang/Desktop/chromedriver')
import time

driver.get("https://bj.lianjia.com/ershoufang/")


   

driver.find_element_by_link_text("2").click()



html = driver.page_source

soup = BeautifulSoup(html,'html.parser')

infos = soup.find('ul',{'class','sellListContent'}).find_all('li')


for info in infos:
    name = info.find('div',{'class':'title'}).find('a').get_text()
    print(name)
    totalPrice = info.find('div',{'class':'priceInfo'}).find('div',{'class':'totalPrice'}).find('span').get_text()
    print(totalPrice)
    unitPrice = info.find('div',{'class':'priceInfo'}).find('div',{'class':'unitPrice'}).find('span').get_text()
    print(unitPrice)
    address = info.find('div',{'class','address'}).find('div',{'class','houseInfo'}).get_text()
    print(address)
    with open('lianjia.csv','a',encoding='utf-8') as f:
        f.write("{},{},{},{}\n".format(name,totalPrice,unitPrice,address))

```



> 使用Beautiful爬取链家网站信息



```python
import requests
import time
url = 'https://bj.lianjia.com/ershoufang/pg3/'

page = 100
with open('lianjia.csv','a',encoding='utf-8') as f:
    for i in range(page):
        time.sleep(3)
        url = 'https://bj.lianjia.com/ershoufang/pg'+str(i)


        headers={
            'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'
        }

        resp = requests.get(url,headers=headers)


        # print(resp.content.decode('utf-8'))  网页内容 二进制
        # print(resp.text) 网络内容 文本

        soup = BeautifulSoup(resp.text,'lxml')

        infos = soup.find('ul',{'class','sellListContent'}).find_all('li')



        for info in infos:
            name = info.find('div',{'class':'title'}).find('a').get_text()
            #print(name)
             
            totalPrice = info.find('div',{'class':'priceInfo'}).find('div',{'class':'totalPrice'}).find('span').get_text()
            #print(totalPrice)
            unitPrice = info.find('div',{'class':'priceInfo'}).find('div',{'class':'unitPrice'}).find('span').get_text()
            #print(unitPrice)
            #address = info.find('div',{'class','address'}).find('div',{'class','houseInfo'}).get_text()
           # print(address)

            list = address.split("|")
            
            
            f.write("{},{},{},{}\n".format(name,totalPrice,unitPrice))
```