---
layout: post
author: LIU,HONGYANG
tags: [Hadoop]
---





# HDFS

## 第1.1节 HDFS基础

### HDFS定义：

HDFS是分布式文件管理系统， 用于存储文件

特点：

分布式的、目录树来定位



 HDFS使用场景：

- 一次写入、多次读出
- 不支持文件修改



### HDFS优缺点：

#### 优点：

- 数据自动保存多个副本，一个副本丢失以后，它可以自动恢复

- 适合处理大数据
- 可构建在廉价机器上



#### 缺点：

- 不适合低延时数据访问
- 无法高效对小文件进行存储
  - NameNode大小有限，而一个NameNode的metadata为150字节，会占用大量NameNode空间
  - 小文件存储的寻址时间会超过读取时间，违反了HDFS设计原则
- 不支持并发写入，文件随机修改
  - 一个文件只能有一个写，不允许有多个写
  - 仅支持数据的append(追加)，不支持文件的随机修改





### HDFS组成架构：







![hdfsarchitecture](https://tva1.sinaimg.cn/large/007S8ZIlgy1gdyxtlism7j30oa0gsjs7.jpg)



#### SecondaryNameNode:

并非NameNode的备份，作用: 合并Fsimage和Edits，并推进给NameNode.

dfs.blocksize: 134417728（默认的NameNode块大小）





### HDFS文件块大小设置：



寻址时间约为10ms，即查找到目标block块时间为10ms

寻址时间是文件传输时间的1%，则为最佳

因此

传输时间为10ms / 0.01 = 1000 ms = 1s



目前磁盘的传输速度普遍为100MB/S



为什么块的大小不能设置太大，也不能设置太小？



（1）HDFS的块设置太小，会增加寻址时间，程序一直找块的位置



（2）如果块设置太大，从磁盘传输数据的时间会明显大于定位这个块位置所需时间，导致程序在处理这块数据时，会非常慢。





**总结：HDFS块的大小主要取决于磁盘的传输速率**





### HDFS SHELL 命令：



##### 查看文件ls：



```shell
hadoop fs -ls /
```



##### 迭代查看文件ls -R：



```shell
hadoop fs -lsr /
```

或者

```shell
hadoop fs -ls -R /
```



##### 创建目录mkdir：



```
hadoop fs -mkdir -p /new/test1
```



注意： -p不能丢



##### 从本地移动文件moveFromLocal：

使用`moveFromLocal`命令移动文件从本地到/leo/test1目录下：

```
hadoop fs -moveFromLocal file.txt /leo/test1
```



![image-20200419154748991](https://tva1.sinaimg.cn/large/007S8ZIlgy1gdz4igkm1gj31z40nc0vj.jpg)





##### 从本地追加文件appendFile：

```shell
hadoop fs -appendToFile liubei.txt /leo/test1/file.txt
```



![image-20200419155900477](https://tva1.sinaimg.cn/large/007S8ZIlgy1gdz4uznrezj31z40nejts.jpg)



##### 查看文件内容cat：



```shell
hadoop fs -cat /leo/test1/file.txt
```



![image-20200419155939894](https://tva1.sinaimg.cn/large/007S8ZIlgy1gdz4uxq1eyj30vi0j074w.jpg)





##### 修改文件权限chgrp：



`-chgrp`  `-chmod` ` -chown`

Linux文件系统中一样，修改文件权限：



```shell
hadoop fs -chgrp leo1 /leo/test1/file.txt
```



![image-20200419160534033](https://tva1.sinaimg.cn/large/007S8ZIlgy1gdz50xc184j31te04uaat.jpg)



![image-20200419160550560](https://tva1.sinaimg.cn/large/007S8ZIlgy1gdz517bw8nj31u404ydgj.jpg)





##### 从本地复制文件copyFromLocal:

等同于`-put`

```
hadoop fs copyFromLocal test.txt /
```





##### 从HDFS复制文件到本地copyToLocal:

等同于`-get`

```shell
hadoop fs copyToLocal /leo/test1/file.txt ./
```





##### 在HDFS上复制文件cp：

```
hadoop fs -cp /leo/test1/file.txt /leo/test2
```



![image-20200419161255162](https://tva1.sinaimg.cn/large/007S8ZIlgy1gdz58kbsfjj31ry024mxf.jpg)





##### 在HDFS上移动文件mv：



```
hadoop fs -mv /leo/test1/file.txt /leo
```



##### 从 HDFS上下载多个文件并合并getmerge：



将test1目录下多个文件合并成hebing.txt

```
hadoop fs -getmerge /leo/test1/* ./hebing.txt
```



##### 查看HDFS文件末尾数据tail:



```shell
hadoop fs -tail /leo/test1/file.txt
```



##### 删除HDFS文件rm：

```
hadoop fs -rm 
hadoop fs -rm -r /test
```



##### 删除HDFS空目录rmdir：

```
hadoop fs -rmdir /*
```



##### 查看HDFS文件信息大小du：

```shell
hadoop fs -du -h /
```



```shell
hadoop fs -du -h -s /
```



![image-20200419163914694](https://tva1.sinaimg.cn/large/007S8ZIlgy1gdz5zypyiej30j001yglp.jpg)





##### 设置文件副本数setrep:



```shell
hadoop fs -setrep 2
```



文件存储路径：

```
~/data/dataNode/current/BP-391230413-172.16.12.60-1587097259981/current/finalized/subdir0/subdir0$
```





### 客户端准备：



```java
package com.hdfs;


import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.io.IOException;
import java.net.URI;

public class hdfs01 {
    public static void main(String[] args) throws IOException, InterruptedException {


        //1.获取hdfs客户端请求
        Configuration conf = new Configuration();
        FileSystem fileSystem = FileSystem.get(URI.create("hdfs://121.41.86.:9000"),conf,"liu");


        //2.在hdfs上创建文件路径
        fileSystem.mkdirs(new Path("/hdfs/test1"));

        //3.关闭资源

        fileSystem.close();

        System.out.println("over");

    }
}


```







## 第1.2节 HDFS API操作





HDFS文件上传



```java
   //1.文件上传
    public static void testCopyFromLocalFile() throws IOException, InterruptedException {
        Configuration conf = new Configuration();
        //1.获取fs对象
        FileSystem fs = FileSystem.get(URI.create("hdfs://alicloud:9000"),conf,"liu" );

        //2.执行上传操作
        fs.copyFromLocalFile(new Path("/Users/liuhongyang/Desktop/input.txt"),new Path("/input.txt"));

        //3.关闭资源
        fs.close();
    }
```



